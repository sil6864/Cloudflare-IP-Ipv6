# -*- coding: utf-8 -*-
"""
Cloudflare IPv6ä¼˜é€‰IPè‡ªåŠ¨æŠ“å–è„šæœ¬
--------------------------------
- ä¸“é—¨æŠ“å–IPv6åœ°å€ï¼Œæ’é™¤IPv4
- æ”¯æŒé™æ€/åŠ¨æ€ç½‘é¡µæŠ“å–ï¼Œè‡ªåŠ¨å»é‡ã€æ’åºã€åœ°åŒºè¿‡æ»¤ã€æ’é™¤ç­‰åŠŸèƒ½
- é…ç½®çµæ´»ï¼Œæ”¯æŒå¤šæ•°æ®æºã€CSSé€‰æ‹©å™¨ã€IPæ•°é‡é™åˆ¶ã€åœ°åŒºAPIç­‰
- æ—¥å¿—è¯¦ç»†ï¼Œå¼‚å¸¸å¤„ç†å¥å£®ï¼Œå…¼å®¹å¤šå¹³å°
- æ–°å¢IPæœ‰æ•ˆæ€§éªŒè¯ï¼Œè¿‡æ»¤æ— æ•ˆåœ°å€
"""

# ===== æ ‡å‡†åº“å¯¼å…¥ =====
import os
import re
import logging
import time
from typing import List, Set, Optional, Dict, Any, Union, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
import ipaddress
import threading
from functools import wraps
import json

# ===== ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ =====
try:
    import requests
    from requests.adapters import HTTPAdapter, Retry
    from bs4 import BeautifulSoup
    from playwright.sync_api import sync_playwright, Page
    import asyncio
    import aiohttp
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–: {e}. è¯·å…ˆè¿è¡Œ pip install -r requirements.txt å¹¶å®‰è£…playwrightæµè§ˆå™¨ã€‚")
    raise

# ===== å¯é€‰ä¾èµ–ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰ =====
try:
    import yaml
except ImportError:
    yaml = None
    print("æœªæ£€æµ‹åˆ° PyYAMLï¼Œè¯·å…ˆè¿è¡Œ pip install pyyaml")

# ===== å¸¸é‡å®šä¹‰ =====
USER_AGENT: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
DEFAULT_JS_TIMEOUT: int = 30000
DEFAULT_WAIT_TIMEOUT: int = 5000
MIN_IP_BLOCK: int = 3
MAX_THREAD_NUM: int = 4

# ===== é…ç½®åŠ è½½å‡½æ•° =====
def load_config(config_path: str = 'config.yaml') -> Dict[str, Any]:
    """
    è¯»å–å¹¶æ ¡éªŒ config.yaml é…ç½®æ–‡ä»¶ã€‚
    :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    :return: é…ç½®å­—å…¸
    :raises RuntimeError, FileNotFoundError, ValueError, KeyError: é…ç½®å¼‚å¸¸
    """
    if not yaml:
        raise RuntimeError('æœªæ£€æµ‹åˆ° PyYAMLï¼Œè¯·å…ˆè¿è¡Œ pip install pyyaml')
    if not os.path.exists(config_path):
        raise FileNotFoundError('æœªæ‰¾åˆ° config.yaml é…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»º')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            if not isinstance(config, dict):
                raise ValueError('config.yaml æ ¼å¼é”™è¯¯ï¼Œéœ€ä¸ºå­—å…¸ç»“æ„')
            
            # å¿…éœ€å­—æ®µæ£€æŸ¥
            required = ['sources', 'pattern', 'output', 'timeout', 'log', 'max_workers', 'log_level', 'js_retry', 'js_retry_interval']
            for k in required:
                if k not in config:
                    raise KeyError(f'config.yaml ç¼ºå°‘å¿…éœ€å­—æ®µ: {k}')
            
            # å…¼å®¹sourcesä¸ºå­—ç¬¦ä¸²æˆ–å­—å…¸
            new_sources = []
            for item in config['sources']:
                if isinstance(item, str):
                    new_sources.append({
                        'url': item, 
                        'selector': None,
                        'page_type': None,
                        'wait_time': DEFAULT_WAIT_TIMEOUT,
                        'actions': None,
                        'extra_headers': None,
                        'response_format': None,
                        'json_path': None
                    })
                elif isinstance(item, dict):
                    new_sources.append({
                        'url': item['url'], 
                        'selector': item.get('selector'),
                        'page_type': item.get('page_type'),
                        'wait_time': item.get('wait_time', DEFAULT_WAIT_TIMEOUT),
                        'actions': item.get('actions'),
                        'extra_headers': item.get('extra_headers'),
                        'response_format': item.get('response_format'),
                        'json_path': item.get('json_path')
                    })
                else:
                    raise ValueError('sources åˆ—è¡¨å…ƒç´ å¿…é¡»ä¸ºå­—ç¬¦ä¸²æˆ–åŒ…å«url/selectorçš„å­—å…¸')
            config['sources'] = new_sources
            
            # è®¾ç½®é»˜è®¤å€¼
            config.setdefault('max_ips_per_url', 0)
            config.setdefault('per_url_limit_mode', 'random')
            config.setdefault('exclude_ips', [])
            config.setdefault('allowed_regions', [])
            config.setdefault('ip_geo_api', '')
            config.setdefault('auto_detect', True)
            config.setdefault('xpath_support', False)
            config.setdefault('follow_redirects', True)
            config.setdefault('enable_telegram_notification', False)
            
            return config
    except Exception as e:
        raise RuntimeError(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

# ---------------- æ—¥å¿—é…ç½® ----------------
def setup_logging(log_file: str, log_level: str = 'INFO') -> None:
    """
    é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ã€‚
    :param log_file: æ—¥å¿—æ–‡ä»¶å
    :param log_level: æ—¥å¿—ç­‰çº§ï¼ˆå¦‚INFOã€DEBUGç­‰ï¼‰
    """
    level = getattr(logging, log_level.upper(), logging.INFO)
    logging.basicConfig(
        level=level,
        format='%(asctime)s %(levelname)s %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

# ---------------- å·¥å…·å‡½æ•° ----------------
def is_valid_ipv6(ip_str: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„IPv6åœ°å€"""
    try:
        ipaddress.IPv6Address(ip_str)
        return True
    except ipaddress.AddressValueError:
        return False

def extract_ips(text: str, pattern: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰IPåœ°å€ï¼Œå¹¶ä¿æŒåŸå§‹é¡ºåºã€‚
    :param text: è¾“å…¥æ–‡æœ¬
    :param pattern: IPæ­£åˆ™è¡¨è¾¾å¼
    :return: IPåˆ—è¡¨ (æŒ‰æ‰¾åˆ°çš„é¡ºåº)
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰IPï¼Œé¡ºåºä¸åŸæ–‡ä¸€è‡´
    raw_ips = re.findall(pattern, text)
    # è¿‡æ»¤æ— æ•ˆIPåœ°å€
    valid_ips = [ip for ip in raw_ips if is_valid_ipv6(ip)]
    invalid_count = len(raw_ips) - len(valid_ips)
    if invalid_count > 0:
        logging.warning(f"[VALIDATION] å‘ç° {invalid_count} ä¸ªæ— æ•ˆIPåœ°å€å·²è¢«è¿‡æ»¤")
    return valid_ips

def save_ips(ip_list: List[str], filename: str) -> None:
    """
    ä¿å­˜IPåˆ—è¡¨åˆ°æ–‡ä»¶ï¼Œä¿æŒé¡ºåºã€‚
    :param ip_list: IPåˆ—è¡¨
    :param filename: è¾“å‡ºæ–‡ä»¶å
    """
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            for ip in ip_list:
                file.write(ip + '\n')
        logging.info(f"å…±ä¿å­˜ {len(ip_list)} ä¸ªå”¯ä¸€IPåˆ° {filename}")
    except Exception as e:
        logging.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {filename}ï¼Œé”™è¯¯: {e}")

# ---------------- requestsé‡è¯•é…ç½® ----------------
def get_retry_session(timeout: int) -> requests.Session:
    """
    è·å–å¸¦é‡è¯•æœºåˆ¶çš„requests.Sessionã€‚
    :param timeout: è¶…æ—¶æ—¶é—´
    :return: é…ç½®å¥½çš„Session
    """
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    session.request = lambda *args, **kwargs: requests.Session.request(session, *args, timeout=timeout, **kwargs)
    return session

# ---------------- æ™ºèƒ½æŠ“å– ----------------
def extract_ips_from_html(html: str, pattern: str, selector: Optional[str] = None) -> List[str]:
    """
    æ™ºèƒ½æå–IPï¼Œä¼˜å…ˆç”¨selectorï¼Œå…¶æ¬¡è‡ªåŠ¨æ£€æµ‹IPå¯†é›†å—ï¼Œæœ€åå…¨å±€éå†ã€‚
    :param html: ç½‘é¡µHTML
    :param pattern: IPæ­£åˆ™
    :param selector: å¯é€‰ï¼ŒCSSé€‰æ‹©å™¨
    :return: IPåˆ—è¡¨ï¼ˆé¡ºåºä¸é¡µé¢ä¸€è‡´ï¼‰
    """
    soup = BeautifulSoup(html, 'html.parser')
    # 1. ä¼˜å…ˆç”¨selector
    if selector:
        selected = soup.select(selector)
        if selected:
            ip_list = []
            for elem in selected:
                ip_list.extend(extract_ips(elem.get_text(), pattern))
            if ip_list:
                logging.info(f"[EXTRACT] ä½¿ç”¨selector '{selector}' æå–åˆ°{len(ip_list)}ä¸ªIP")
                return list(dict.fromkeys(ip_list))
    
    # 2. è‡ªåŠ¨æ£€æµ‹IPå¯†é›†å—
    candidates = []
    for tag in ['pre', 'code', 'table', 'div', 'section', 'article']:
        for elem in soup.find_all(tag):
            text = elem.get_text()
            ips = extract_ips(text, pattern)
            if len(ips) >= MIN_IP_BLOCK:
                candidates.append((len(ips), ips))
    
    if candidates:
        candidates.sort(reverse=True)
        ip_list = candidates[0][1]
        logging.info(f"[EXTRACT] è‡ªåŠ¨æ£€æµ‹åˆ°IPå¯†é›†å—({len(ip_list)}ä¸ªIP, tagä¼˜å…ˆçº§)")
        return list(dict.fromkeys(ip_list))
    
    # 3. å…¨å±€éå†
    all_text = soup.get_text()
    ip_list = extract_ips(all_text, pattern)
    logging.info(f"[EXTRACT] å…¨å±€éå†æå–åˆ°{len(ip_list)}ä¸ªIP")
    return list(dict.fromkeys(ip_list))

def fetch_ip_auto(
    url: str,
    pattern: str,
    timeout: int,
    session: requests.Session,
    page: Optional[Page] = None,
    js_retry: int = 3,
    js_retry_interval: float = 2.0,
    selector: Optional[str] = None
) -> List[str]:
    """
    æ™ºèƒ½è‡ªåŠ¨æŠ“å–IPï¼Œä¼˜å…ˆé™æ€ï¼Œå¤±è´¥è‡ªåŠ¨åˆ‡æ¢JSåŠ¨æ€ã€‚
    :param url: ç›®æ ‡URL
    :param pattern: IPæ­£åˆ™
    :param timeout: è¶…æ—¶æ—¶é—´
    :param session: requests.Session
    :param page: Playwrighté¡µé¢å¯¹è±¡
    :param js_retry: JSåŠ¨æ€é‡è¯•æ¬¡æ•°
    :param js_retry_interval: JSé‡è¯•é—´éš”
    :param selector: CSSé€‰æ‹©å™¨
    :return: IPåˆ—è¡¨
    """
    logging.info(f"[AUTO] æ­£åœ¨æŠ“å–: {url}")
    extracted_ips: List[str] = []
    
    # å°è¯•é™æ€æŠ“å–
    try:
        headers = {"User-Agent": USER_AGENT}
        response = session.get(url, headers=headers)
        response.raise_for_status()
        text = response.text
        extracted_ips = extract_ips_from_html(text, pattern, selector)
        if extracted_ips:
            logging.info(f"[AUTO] é™æ€æŠ“å–æˆåŠŸ: {url}ï¼Œå…±{len(extracted_ips)}ä¸ªIP")
            return extracted_ips
        else:
            logging.info(f"[AUTO] é™æ€æŠ“å–æ— IPï¼Œå°è¯•JSåŠ¨æ€: {url}")
    except requests.RequestException as e:
        logging.warning(f"[AUTO] é™æ€æŠ“å–å¤±è´¥: {url}ï¼Œç½‘ç»œé”™è¯¯: {e}ï¼Œå°è¯•JSåŠ¨æ€")
    except Exception as e:
        logging.warning(f"[AUTO] é™æ€æŠ“å–å¤±è´¥: {url}ï¼Œè§£æé”™è¯¯: {e}ï¼Œå°è¯•JSåŠ¨æ€")
    
    # å°è¯•JSåŠ¨æ€æŠ“å–
    if page is not None:
        try:
            page.set_extra_http_headers({"User-Agent": USER_AGENT})
        except Exception:
            pass
        
        found_ip_list = []
        
        # ç›‘å¬å“åº”è·å–IP
        def handle_response(response):
            try:
                text = response.text()
                ip_list = extract_ips(text, pattern)
                if len(ip_list) >= MIN_IP_BLOCK:
                    found_ip_list.extend(ip_list)
            except Exception:
                pass
        
        page.on("response", handle_response)
        
        for attempt in range(1, js_retry + 1):
            try:
                page.goto(url, timeout=DEFAULT_JS_TIMEOUT)
                page.wait_for_timeout(DEFAULT_WAIT_TIMEOUT)
                
                if found_ip_list:
                    found_ip_list = list(dict.fromkeys(found_ip_list))
                    logging.info(f"[AUTO] ç›‘å¬æ¥å£è‡ªåŠ¨æå–åˆ° {len(found_ip_list)} ä¸ªIP: {found_ip_list[:10]}")
                    return found_ip_list
                
                # è·å–é¡µé¢å†…å®¹
                page_content = page.content()
                
                # å°è¯•ä»è¡¨æ ¼ä¸­æå–IP
                if '<table' in page_content.lower():
                    soup = BeautifulSoup(page_content, 'html.parser')
                    ip_list: List[str] = []
                    table = soup.find('table')
                    if table:
                        for row in table.find_all('tr'):
                            for cell in row.find_all('td'):
                                ip_list.extend(extract_ips(cell.get_text(), pattern))
                    else:
                        elements = soup.find_all('tr') if soup.find_all('tr') else soup.find_all('li')
                        for element in elements:
                            ip_list.extend(extract_ips(element.get_text(), pattern))
                    extracted_ips = list(dict.fromkeys(ip_list))
                    logging.info(f"[DEBUG] {url} JSåŠ¨æ€è¡¨æ ¼æå–å‰10ä¸ªIP: {extracted_ips[:10]}")
                else:
                    ip_list = extract_ips(page_content, pattern)
                    extracted_ips = list(dict.fromkeys(ip_list))
                    logging.info(f"[DEBUG] {url} JSåŠ¨æ€çº¯æ–‡æœ¬å‰10ä¸ªIP: {extracted_ips[:10]}")
                
                if extracted_ips:
                    logging.info(f"[AUTO] JSåŠ¨æ€æŠ“å–æˆåŠŸ: {url}ï¼Œå…±{len(extracted_ips)}ä¸ªIP")
                    return extracted_ips
                else:
                    logging.warning(f"[AUTO] JSåŠ¨æ€æŠ“å–æ— IP: {url}ï¼Œç¬¬{attempt}æ¬¡")
            except Exception as e:
                logging.error(f"[AUTO] JSåŠ¨æ€æŠ“å–å¤±è´¥: {url}ï¼Œç¬¬{attempt}æ¬¡ï¼Œé”™è¯¯: {e}")
            
            if attempt < js_retry:
                time.sleep(js_retry_interval)
        
        logging.error(f"[AUTO] JSåŠ¨æ€æŠ“å–å¤šæ¬¡å¤±è´¥: {url}")
    else:
        logging.error(f"[AUTO] æœªæä¾›pageå¯¹è±¡ï¼Œæ— æ³•è¿›è¡ŒJSåŠ¨æ€æŠ“å–: {url}")
    
    return []

async def fetch_ip_static_async(url: str, pattern: str, timeout: int, session: aiohttp.ClientSession, selector: Optional[str] = None) -> tuple[str, List[str], bool]:
    """
    å¼‚æ­¥é™æ€é¡µé¢æŠ“å–ä»»åŠ¡ï¼Œè¿”å›(url, IPåˆ—è¡¨ (æœ‰åºä¸”å”¯ä¸€), æ˜¯å¦æˆåŠŸ)ã€‚
    :param url: ç›®æ ‡URL
    :param pattern: IPæ­£åˆ™
    :param timeout: è¶…æ—¶æ—¶é—´
    :param session: aiohttp.ClientSession
    :param selector: å¯é€‰ï¼ŒCSSé€‰æ‹©å™¨
    :return: (url, IPåˆ—è¡¨ (æœ‰åºä¸”å”¯ä¸€), æ˜¯å¦æˆåŠŸ)
    """
    try:
        headers = {"User-Agent": USER_AGENT}
        async with session.get(url, timeout=timeout, headers=headers) as response:
            if response.status != 200:
                logging.warning(f"[ASYNC] é™æ€æŠ“å–å¤±è´¥: {url}ï¼ŒHTTPçŠ¶æ€ç : {response.status}")
                return (url, [], False)
            
            text = await response.text()
            ordered_unique_ips: List[str] = extract_ips_from_html(text, pattern, selector)
            
            if ordered_unique_ips:
                logging.info(f"[ASYNC] é™æ€æŠ“å–æˆåŠŸ: {url}ï¼Œå…±{len(ordered_unique_ips)}ä¸ªIP")
                return (url, ordered_unique_ips, True)
            else:
                logging.info(f"[ASYNC] é™æ€æŠ“å–æ— IPï¼ŒåŠ å…¥JSåŠ¨æ€é˜Ÿåˆ—: {url}")
                return (url, [], False)
    except asyncio.TimeoutError:
        logging.warning(f"[ASYNC] é™æ€æŠ“å–è¶…æ—¶: {url}ï¼ŒåŠ å…¥JSåŠ¨æ€é˜Ÿåˆ—")
        return (url, [], False)
    except Exception as e:
        logging.warning(f"[ASYNC] é™æ€æŠ“å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}ï¼ŒåŠ å…¥JSåŠ¨æ€é˜Ÿåˆ—")
        return (url, [], False)

# ---------------- æ–°å¢ï¼šIPæ•°é‡é™åˆ¶ ----------------
def limit_ips(ip_collection: Union[List[str], Set[str]], max_count: int, mode: str = 'random') -> List[str]:
    """
    é™åˆ¶IPé›†åˆ/åˆ—è¡¨çš„æ•°é‡ï¼Œæ ¹æ®æŒ‡å®šæ¨¡å¼è¿”å›æœ‰é™çš„IPåˆ—è¡¨ï¼ˆæœ‰åºï¼‰ã€‚
    :param ip_collection: åŸå§‹IPåˆ—è¡¨ (ç”¨äºtopæ¨¡å¼ï¼Œéœ€ä¿æŒé¡ºåº) æˆ–é›†åˆ (ç”¨äºrandomæ¨¡å¼)
    :param max_count: æœ€å¤§ä¿ç•™æ•°é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
    :param mode: é™åˆ¶æ¨¡å¼ï¼Œ'random'ä¸ºéšæœºä¿ç•™ï¼Œ'top'ä¸ºä¿ç•™é¡µé¢é å‰çš„
    :return: é™åˆ¶åçš„IPåˆ—è¡¨ï¼ˆæœ‰åºï¼‰
    """
    collection_list = list(ip_collection)
    collection_len = len(collection_list)
    if max_count <= 0 or collection_len <= max_count:
        return collection_list
    
    if mode == 'top':
        return collection_list[:max_count]
    elif mode == 'random':
        import random
        return random.sample(collection_list, max_count)
    else:
        logging.warning(f"[LIMIT] æœªçŸ¥çš„é™åˆ¶æ¨¡å¼: {mode}ï¼Œä½¿ç”¨é»˜è®¤çš„éšæœºæ¨¡å¼")
        import random
        return random.sample(collection_list, max_count)

async def async_static_crawl(sources: List[Dict[str, str]], pattern: str, timeout: int, max_ips: int = 0, limit_mode: str = 'random') -> tuple[Dict[str, List[str]], List[str]]:
    """
    å¹¶å‘æŠ“å–æ‰€æœ‰é™æ€é¡µé¢ï¼Œè¿”å›æ¯ä¸ªURLçš„IPåˆ—è¡¨å’Œéœ€è¦JSåŠ¨æ€æŠ“å–çš„URLã€‚
    :param sources: [{url, selector}]åˆ—è¡¨
    :param pattern: IPæ­£åˆ™
    :param timeout: è¶…æ—¶æ—¶é—´
    :param max_ips: æ¯ä¸ªURLæœ€å¤šä¿ç•™çš„IPæ•°é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
    :param limit_mode: é™åˆ¶æ¨¡å¼ï¼Œ'random'ä¸ºéšæœºä¿ç•™ï¼Œ'top'ä¸ºä¿ç•™é¡µé¢é å‰çš„
    :return: (æ¯ä¸ªURLçš„IPåˆ—è¡¨å­—å…¸, éœ€è¦JSåŠ¨æ€æŠ“å–çš„URLåˆ—è¡¨)
    """
    url_ips_dict: Dict[str, List[str]] = {}
    need_js_urls: List[str] = []
    
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [fetch_ip_static_async(item['url'], pattern, timeout, session, item.get('selector')) for item in sources]
        results = await asyncio.gather(*tasks)
        
        for url, fetched_ip_list, success in results:
            if success:
                processed_ips_list: List[str]
                if max_ips > 0 and len(fetched_ip_list) > max_ips:
                    original_count = len(fetched_ip_list)
                    processed_ips_list = limit_ips(fetched_ip_list, max_ips, limit_mode)
                    logging.info(f"[LIMIT] URL {url} IPæ•°é‡ä» {original_count} é™åˆ¶ä¸º {len(processed_ips_list)}")
                else:
                    processed_ips_list = fetched_ip_list
                url_ips_dict[url] = processed_ips_list
            else:
                need_js_urls.append(url)
    
    return url_ips_dict, need_js_urls

# ---------------- æ–°å¢ï¼šIPæ’é™¤åŠŸèƒ½ ----------------
def build_ip_exclude_checker(exclude_patterns: List[str]) -> Callable[[str], bool]:
    """
    æ„å»ºIPæ’é™¤æ£€æŸ¥å™¨ï¼Œæ”¯æŒç²¾ç¡®åŒ¹é…å’ŒCIDRæ ¼å¼ç½‘æ®µåŒ¹é…ã€‚
    :param exclude_patterns: æ’é™¤IP/ç½‘æ®µåˆ—è¡¨
    :return: æ£€æŸ¥å‡½æ•°ï¼Œæ¥æ”¶IPå­—ç¬¦ä¸²ï¼Œè¿”å›æ˜¯å¦åº”è¯¥æ’é™¤
    """
    if not exclude_patterns:
        return lambda ip: False
    
    # é¢„å¤„ç†æ’é™¤åˆ—è¡¨
    exact_ips = set()
    networks = []
    
    for pattern in exclude_patterns:
        pattern = pattern.strip()
        if '/' in pattern:
            try:
                networks.append(ipaddress.ip_network(pattern, strict=False))
            except ValueError as e:
                logging.warning(f"æ— æ•ˆçš„CIDRæ ¼å¼ç½‘æ®µ: {pattern}, é”™è¯¯: {e}")
        else:
            exact_ips.add(pattern)
    
    def is_excluded(ip: str) -> bool:
        """
        æ£€æŸ¥IPæ˜¯å¦åº”è¢«æ’é™¤ã€‚
        :param ip: IPåœ°å€å­—ç¬¦ä¸²
        :return: å¦‚æœåº”è¯¥æ’é™¤åˆ™ä¸ºTrueï¼Œå¦åˆ™ä¸ºFalse
        """
        if ip in exact_ips:
            return True
        
        if networks:
            try:
                ip_obj = ipaddress.ip_address(ip)
                return any(ip_obj in network for network in networks)
            except ValueError:
                logging.warning(f"æ— æ•ˆçš„IPåœ°å€: {ip}")
        
        return False
    
    return is_excluded

# é€Ÿç‡é™åˆ¶è£…é¥°å™¨ï¼ˆæ¯ç§’æœ€å¤šNæ¬¡ï¼‰
def rate_limited(max_per_second: int):
    """
    é€Ÿç‡é™åˆ¶è£…é¥°å™¨ï¼ˆæ¯ç§’æœ€å¤šNæ¬¡ï¼‰ã€‚
    :param max_per_second: æ¯ç§’æœ€å¤§è°ƒç”¨æ¬¡æ•°
    :return: è£…é¥°å™¨
    """
    min_interval = 1.0 / float(max_per_second)
    lock = threading.Lock()
    last_time = [0.0]
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            with lock:
                elapsed = time.time() - last_time[0]
                wait = min_interval - elapsed
                if wait > 0:
                    time.sleep(wait)
                result = func(*args, **kwargs)
                last_time[0] = time.time()
                return result
        return wrapper
    return decorator

# ---------------- æ–°å¢ï¼šåœ°åŒºè¿‡æ»¤ç›¸å…³å‡½æ•° ----------------
@rate_limited(5)  # é»˜è®¤æ¯ç§’æœ€å¤š5æ¬¡
def get_ip_region(ip: str, api_template: str, timeout: int = 5, max_retries: int = 3, retry_interval: float = 1.0) -> str:
    """
    æŸ¥è¯¢IPå½’å±åœ°ï¼Œè¿”å›å›½å®¶/åœ°åŒºä»£ç ï¼ˆå¦‚CNã€USç­‰ï¼‰ï¼Œå¢åŠ é‡è¯•å’Œé™çº§æœºåˆ¶ã€‚
    :param ip: IPåœ°å€
    :param api_template: APIæ¨¡æ¿ï¼Œ{ip}ä¼šè¢«æ›¿æ¢
    :param timeout: è¶…æ—¶æ—¶é—´
    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :param retry_interval: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    :return: å›½å®¶/åœ°åŒºä»£ç ï¼ˆå¤§å†™ï¼‰ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not api_template:
        return ''
    
    url = api_template.replace('{ip}', ip)
    for attempt in range(1, max_retries + 1):
        try:
            resp = requests.get(url, timeout=timeout)
            resp.raise_for_status()
            data = resp.json()
            
            # å…¼å®¹å¸¸è§APIè¿”å›æ ¼å¼
            for key in ['countryCode', 'country_code', 'country', 'countrycode']:
                if key in data:
                    val = data[key]
                    if isinstance(val, str) and len(val) <= 3:
                        return val.upper()
            
            # ipinfo.ioç­‰
            if 'country' in data and isinstance(data['country'], str):
                return data['country'].upper()
        except Exception as e:
            logging.warning(f"[REGION] æŸ¥è¯¢IPå½’å±åœ°å¤±è´¥: {ip}, ç¬¬{attempt}æ¬¡, é”™è¯¯: {e}")
            if attempt < max_retries:
                time.sleep(retry_interval)
    
    return ''

def filter_ips_by_region(ip_list: List[str], allowed_regions: List[str], api_template: str, timeout: int = 5) -> List[str]:
    """
    åªä¿ç•™æŒ‡å®šåœ°åŒºçš„IPï¼Œä¿æŒé¡ºåºã€‚
    :param ip_list: åŸå§‹IPåˆ—è¡¨
    :param allowed_regions: å…è®¸çš„åœ°åŒºä»£ç åˆ—è¡¨
    :param api_template: å½’å±åœ°APIæ¨¡æ¿
    :param timeout: æŸ¥è¯¢è¶…æ—¶æ—¶é—´
    :return: è¿‡æ»¤åçš„IPåˆ—è¡¨
    """
    if not allowed_regions or not api_template:
        return ip_list
    
    allowed_set = set([r.upper() for r in allowed_regions if isinstance(r, str)])
    filtered = []
    
    for ip in ip_list:
        region = get_ip_region(ip, api_template, timeout, max_retries=3, retry_interval=1.0)
        if region in allowed_set:
            filtered.append(ip)
        else:
            logging.info(f"[REGION] è¿‡æ»¤æ‰IP: {ip}ï¼Œå½’å±åœ°: {region if region else 'æœªçŸ¥'}")
    
    return filtered

# ---------------- æ–°å¢ï¼šTelegram é€šçŸ¥åŠŸèƒ½ ----------------
def send_telegram_notification(message: str, bot_token: str, chat_id: str) -> bool:
    """
    å‘é€Telegramé€šçŸ¥æ¶ˆæ¯ã€‚
    :param message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
    :param bot_token: Telegram Bot Token
    :param chat_id: Telegram Chat ID
    :return: Trueè¡¨ç¤ºå‘é€æˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    if not bot_token or not chat_id:
        logging.warning("Telegram BOT_TOKEN æˆ– CHAT_ID æœªè®¾ç½®ï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return False
    
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": message,
        "parse_mode": "Markdown"
    }
    
    try:
        response = requests.post(url, json=payload, timeout=10)
        response.raise_for_status()
        logging.info("Telegramé€šçŸ¥å‘é€æˆåŠŸã€‚")
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"å‘é€Telegramé€šçŸ¥å¤±è´¥: {e}")
        return False

# ---------------- æ–°å¢ï¼šä»APIå“åº”ä¸­æå–IP ----------------
def extract_ips_from_api(response_text: str, pattern: str, json_path: Optional[str] = None) -> List[str]:
    """
    ä»APIå“åº”ä¸­æå–IPåœ°å€ã€‚
    :param response_text: APIå“åº”æ–‡æœ¬
    :param pattern: IPæ­£åˆ™è¡¨è¾¾å¼
    :param json_path: JSONè·¯å¾„ï¼Œç”¨äºæå–IPåˆ—è¡¨ï¼Œå¦‚"data.ips"
    :return: IPåˆ—è¡¨ï¼ˆé¡ºåºä¸APIè¿”å›ä¸€è‡´ï¼‰
    """
    try:
        # å°è¯•è§£æä¸ºJSON
        json_data = json.loads(response_text)
        
        # å¦‚æœæŒ‡å®šäº†JSONè·¯å¾„
        if json_path:
            # æŒ‰è·¯å¾„é€å±‚è·å–
            parts = json_path.split('.')
            data = json_data
            for part in parts:
                if isinstance(data, dict) and part in data:
                    data = data[part]
                else:
                    logging.warning(f"[API] JSONè·¯å¾„ {json_path} ä¸­çš„ {part} æœªæ‰¾åˆ°")
                    return []
            
            # æå–IPåˆ—è¡¨
            if isinstance(data, list):
                ip_list = []
                for item in data:
                    if isinstance(item, str) and re.match(pattern, item):
                        ip_list.append(item)
                if ip_list:
                    logging.info(f"[API] ä»JSONè·¯å¾„ {json_path} æå–åˆ° {len(ip_list)} ä¸ªIP")
                    return ip_list
            elif isinstance(data, str):
                ip_list = extract_ips(data, pattern)
                if ip_list:
                    logging.info(f"[API] ä»JSONè·¯å¾„ {json_path} æå–åˆ° {len(ip_list)} ä¸ªIP")
                    return ip_list
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šJSONè·¯å¾„æˆ–è€…æŒ‡å®šè·¯å¾„æå–å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½æå–
        ip_list = extract_ips(response_text, pattern)
        if ip_list:
            logging.info(f"[API] ä»æ•´ä¸ªAPIå“åº”ä¸­æå–åˆ° {len(ip_list)} ä¸ªIP")
            return ip_list
        
        logging.warning("[API] æœªèƒ½ä»APIå“åº”ä¸­æå–åˆ°IP")
        return []
    except json.JSONDecodeError:
        logging.warning("[API] APIå“åº”è§£æJSONå¤±è´¥ï¼Œå°è¯•ç›´æ¥æ­£åˆ™æå–IP")
        ip_list = extract_ips(response_text, pattern)
        if ip_list:
            logging.info(f"[API] ä»éJSONå“åº”ä¸­æå–åˆ° {len(ip_list)} ä¸ªIP")
            return ip_list
        return []
    except Exception as e:
        logging.error(f"[API] æå–IPå¼‚å¸¸: {e}")
        return []

# ---------------- ä¸»æµç¨‹ ----------------
def main() -> None:
    """
    ä¸»ç¨‹åºå…¥å£ï¼Œåªä» config.yaml è¯»å–é…ç½®ï¼Œç¼ºå¤±é¡¹æŠ¥é”™ã€‚
    1. è¯»å–é…ç½®å¹¶æ ¡éªŒ
    2. å¼‚æ­¥å¹¶å‘é™æ€æŠ“å–
    3. Playwright åŠ¨æ€æŠ“å–ï¼ˆå¸¦é‡è¯•ï¼‰
    4. ç»“æœå»é‡å¹¶ä¿å­˜
    """
    try:
        config = load_config()
    except Exception as e:
        logging.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        return
    
    sources = config['sources']
    pattern = config['pattern']
    output = config['output']
    timeout = config['timeout']
    log_file = config['log']
    max_workers = config['max_workers']
    log_level = config['log_level']
    js_retry = config['js_retry']
    js_retry_interval = config['js_retry_interval']
    max_ips_per_url = config['max_ips_per_url']
    per_url_limit_mode = config['per_url_limit_mode']
    exclude_ips_config = config['exclude_ips']
    allowed_regions = config['allowed_regions']
    ip_geo_api = config['ip_geo_api']
    auto_detect = config.get('auto_detect', True)
    follow_redirects = config.get('follow_redirects', True)
    enable_telegram_notification = config.get('enable_telegram_notification', False)

    setup_logging(log_file, log_level)
    logging.info(f"å¼€å§‹æ‰§è¡ŒCloudflare IPv6æŠ“å–ï¼Œè‡ªåŠ¨æ£€æµ‹: {auto_detect}")
    
    if os.path.exists(output):
        try:
            os.remove(output)
        except Exception as e:
            logging.error(f"æ— æ³•åˆ é™¤æ—§çš„è¾“å‡ºæ–‡ä»¶: {output}ï¼Œé”™è¯¯: {e}")

    url_ips_map: Dict[str, List[str]] = {}
    static_sources = []
    dynamic_sources = []
    
    # æ ¹æ®é¡µé¢ç±»å‹åˆ†ç±»æ•°æ®æº
    for source in sources:
        url = source['url']
        page_type = source.get('page_type')
        
        # å¦‚æœæ˜ç¡®æŒ‡å®šä¸ºdynamicæˆ–é…ç½®äº†actionsï¼Œç›´æ¥å½’ä¸ºåŠ¨æ€
        if page_type == 'dynamic' or source.get('actions'):
            dynamic_sources.append(source)
            logging.info(f"URL {url} å·²å½’ç±»ä¸ºåŠ¨æ€æŠ“å–")
        else:
            # å…¶ä»–ç±»å‹å…ˆå°è¯•é™æ€æŠ“å–
            static_sources.append(source)
            logging.info(f"URL {url} å·²å½’ç±»ä¸ºé™æ€æŠ“å–ï¼ˆå¯èƒ½é™çº§ä¸ºåŠ¨æ€ï¼‰")
    
    # åˆ›å»ºå…¨å±€sessionï¼ˆæ”¯æŒé‡å®šå‘ï¼‰
    session = get_retry_session(timeout)
    if follow_redirects:
        session.max_redirects = 5
    
    # å¤„ç†é™æ€æŠ“å–
    for source in static_sources:
        try:
            # ç‰¹æ®Šå¤„ç†JSON API
            if source.get('response_format') == 'json':
                headers = {"User-Agent": USER_AGENT}
                if source.get('extra_headers'):
                    headers.update(source['extra_headers'])
                
                response = session.get(source['url'], headers=headers)
                response.raise_for_status()
                extracted_ips = extract_ips_from_api(response.text, pattern, source.get('json_path'))
            else:
                # æ™®é€šé™æ€é¡µé¢æŠ“å–
                extracted_ips = fetch_ip_auto(
                    source['url'],
                    pattern,
                    timeout,
                    session,
                    None,  # é™æ€æ¨¡å¼ä¸éœ€è¦page
                    js_retry,
                    js_retry_interval,
                    source.get('selector')
                )
            
            if max_ips_per_url > 0 and len(extracted_ips) > max_ips_per_url:
                original_count = len(extracted_ips)
                processed_ips = limit_ips(extracted_ips, max_ips_per_url, per_url_limit_mode)
                logging.info(f"[LIMIT] URL {source['url']} IPæ•°é‡ä» {original_count} é™åˆ¶ä¸º {len(processed_ips)}")
                url_ips_map[source['url']] = processed_ips
            else:
                url_ips_map[source['url']] = extracted_ips
            
            if not extracted_ips:
                # é™æ€æŠ“å–å¤±è´¥ï¼ŒåŠ å…¥åŠ¨æ€é˜Ÿåˆ—
                dynamic_sources.append(source)
                logging.info(f"URL {source['url']} é™æ€æŠ“å–å¤±è´¥ï¼Œå·²åŠ å…¥åŠ¨æ€é˜Ÿåˆ—")
                
        except Exception as e:
            logging.error(f"å¤„ç†é™æ€æºå¼‚å¸¸: {source['url']}, é”™è¯¯: {e}")
            # å‡ºé”™ä¹ŸåŠ å…¥åŠ¨æ€é˜Ÿåˆ—
            dynamic_sources.append(source)
    
    # å¤„ç†åŠ¨æ€æŠ“å–
    if dynamic_sources:
        # ä½¿ç”¨Playwrightå¤„ç†åŠ¨æ€é¡µé¢
        try:
            with sync_playwright() as playwright:
                browser = playwright.chromium.launch(headless=True)
                context = browser.new_context()
                page = context.new_page()
                
                for source in dynamic_sources:
                    try:
                        extracted_ips = fetch_ip_auto(
                            source['url'],
                            pattern,
                            timeout,
                            session,
                            page,
                            js_retry,
                            js_retry_interval,
                            source.get('selector')
                        )
                        
                        if max_ips_per_url > 0 and len(extracted_ips) > max_ips_per_url:
                            original_count = len(extracted_ips)
                            processed_ips = limit_ips(extracted_ips, max_ips_per_url, per_url_limit_mode)
                            logging.info(f"[LIMIT] URL {source['url']} IPæ•°é‡ä» {original_count} é™åˆ¶ä¸º {len(processed_ips)}")
                            url_ips_map[source['url']] = processed_ips
                        else:
                            url_ips_map[source['url']] = extracted_ips
                            
                    except Exception as e:
                        logging.error(f"å¤„ç†åŠ¨æ€æºå¼‚å¸¸: {source['url']}, é”™è¯¯: {e}")
                
                page.close()
                context.close()
                browser.close()
        except Exception as e:
            logging.error(f"Playwrightåˆå§‹åŒ–å¤±è´¥: {e}")

    # æ’é™¤IPå’Œåœ°åŒºè¿‡æ»¤
    is_excluded_func = build_ip_exclude_checker(exclude_ips_config)
    excluded_count = 0

    merged_ips = []
    for url, ips_list_for_url in url_ips_map.items():
        original_count_before_exclude = len(ips_list_for_url)
        retained_ips = [ip for ip in ips_list_for_url if not is_excluded_func(ip)]
        excluded_in_source = original_count_before_exclude - len(retained_ips)
        if excluded_in_source > 0:
            logging.info(f"[EXCLUDE] URL {url} æ’é™¤äº† {excluded_in_source} ä¸ªIPï¼Œä¿ç•™ {len(retained_ips)} ä¸ªIP")
        excluded_count += excluded_in_source
        logging.info(f"URL {url} è´¡çŒ®äº† {len(retained_ips)} ä¸ªIP")
        
        # æ—¥å¿—è¾“å‡ºæ¯ä¸ªURLæœ€ç»ˆç­›é€‰å‡ºæ¥çš„IPï¼ˆå‰20ä¸ªï¼‰
        if len(retained_ips) > 20:
            logging.info(f"[RESULT] URL {url} æœ€ç»ˆç­›é€‰IPï¼ˆå‰20ä¸ªï¼‰: {retained_ips[:20]} ... å…±{len(retained_ips)}ä¸ª")
        else:
            logging.info(f"[RESULT] URL {url} æœ€ç»ˆç­›é€‰IP: {retained_ips}")
        
        merged_ips.extend(retained_ips)

    final_all_ips = list(dict.fromkeys(merged_ips))

    # åœ°åŒºè¿‡æ»¤
    if allowed_regions and ip_geo_api:
        before_region_count = len(final_all_ips)
        final_all_ips = filter_ips_by_region(final_all_ips, allowed_regions, ip_geo_api)
        after_region_count = len(final_all_ips)
        logging.info(f"[REGION] åœ°åŒºè¿‡æ»¤åï¼ŒIPæ•°é‡ä» {before_region_count} é™è‡³ {after_region_count}")

    # ä¿å­˜ç»“æœ
    save_ips(final_all_ips, output)
    logging.info(f"æœ€ç»ˆåˆå¹¶äº† {len(url_ips_map)} ä¸ªURLçš„IPï¼Œæ’é™¤äº† {excluded_count} ä¸ªIPï¼Œå…± {len(final_all_ips)} ä¸ªå”¯ä¸€IPv6åœ°å€")

    # Telegramé€šçŸ¥
    if enable_telegram_notification:
        telegram_bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        telegram_chat_id = os.getenv('TELEGRAM_CHAT_ID')

        if telegram_bot_token and telegram_chat_id:
            notification_message = (
                f"âœ… Cloudflare IPv6ä¼˜é€‰IPæŠ“å–å®Œæˆï¼\n\n"
                f"ğŸ“Š **IPæ•°é‡**: {len(final_all_ips)} ä¸ª\n"
                f"ğŸ—‘ï¸ **æ’é™¤IP**: {excluded_count} ä¸ª\n"
                f"ğŸ’¾ **ä¿å­˜è‡³**: `{output}`\n"
            )
            send_telegram_notification(notification_message, telegram_bot_token, telegram_chat_id)
        else:
            logging.warning("Telegramé€šçŸ¥å·²å¯ç”¨ï¼Œä½†æœªæ‰¾åˆ° TELEGRAM_BOT_TOKEN æˆ– TELEGRAM_CHAT_ID ç¯å¢ƒå˜é‡ã€‚è¯·æ£€æŸ¥GitHub Secretsé…ç½®ã€‚")
    else:
        logging.info("Telegramé€šçŸ¥æœªå¯ç”¨ã€‚")

# ===== ä¸»æµç¨‹å…¥å£ =====
if __name__ == '__main__':
    main()
